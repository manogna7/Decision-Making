{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jtNGqiIKZW"
      },
      "source": [
        "# MDP and Gym Maze Exploration\n",
        "*Created by Jeff Jewett and Will Solow, 2024*\n",
        "\n",
        "*Oregon State AIGSA Reading Group*\n",
        "\n",
        "This Google Colab notebook is made to help guide students through the concept of a Markov Decision Process (MDP) using the example of a grid maze. It also covers setup of a Gymnasium Environment and an implementation of Value Iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and Imports"
      ],
      "metadata": {
        "id": "q35v9Ua_k2bW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G60-chhbH_BW"
      },
      "outputs": [],
      "source": [
        "\"\"\"Install requisite packages\"\"\"\n",
        "!pip install gymnasium[classic-control]\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02tdipXSHZHc"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import packages\"\"\"\n",
        "import io\n",
        "import time\n",
        "\n",
        "import ipywidgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\"\"\"Utility functions\"\"\"\n",
        "def image_to_bytes(img):\n",
        "  if isinstance(img, np.ndarray):\n",
        "    img = Image.fromarray(img, 'RGB')\n",
        "\n",
        "  # Convert the PIL image to bytes\n",
        "  with io.BytesIO() as output:\n",
        "    img.save(output, format=\"PNG\")\n",
        "    image_data = output.getvalue()\n",
        "  return image_data\n",
        "\n",
        "def matrix_to_heatmap_image(matrix, output_size=(512, 512)):\n",
        "  norm_factor = max(np.abs(np.nanmax(matrix)), np.abs(np.nanmin(matrix)))\n",
        "  if norm_factor != 0:\n",
        "    matrix = matrix / norm_factor\n",
        "  cmap = plt.get_cmap('coolwarm')\n",
        "  cmap.set_bad(color='black')\n",
        "  norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "  rgb_array = cmap(norm(matrix))[:, :, :3]\n",
        "  rgb_image = Image.fromarray((rgb_array * 255).astype(np.uint8))\n",
        "\n",
        "  # Resize the image to the specified output size\n",
        "  resized_image = rgb_image.resize(output_size, Image.NEAREST)\n",
        "\n",
        "  return np.array(resized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Process (MDP) for a Maze"
      ],
      "metadata": {
        "id": "bX-fo0gW-fX1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAv0b_nBmfO"
      },
      "source": [
        "We want to represent a maze environment, where there is an agent that can move in the cardinal directions. There are walls which restrict movement. For a given maze of size WIDTHxHEIGHT, our MDP is defined as\n",
        "- S: The state space is {0, 1, 2, ..., WIDTH*HEIGHT-1}, representing the row-wise flattened indices of the maze. For example, for a 5x5 maze, state 8 represents x=1, y=3.\n",
        "- A: The action space is {0, 1, 2, 3}, representing the directions UP, DOWN, LEFT, and RIGHT respectively.\n",
        "- R(S, A) -> ℝ The reward function takes a state and an action and gives a reward. For our simple problem, we give a constant penalty of -1 every action the agent takes. No reward is given at the goal.\n",
        "- P(S, A, S) -> [0,1] The transition function shows the probability of starting at a given state, taking a certain action, and ending up in a different state. Our maze environment is deterministic, so all probabilities are either 0 or 1. For any non-neighbor states S1 and S2, P(S1, ⋅, S2)=0. But if state S3 is to the left of S4, then P(S3, LEFT, S4)=1.\n",
        "- H: The horizon is the maximum amount of actions our agent is allowed to take.\n",
        "- γ (gamma): Gamma is a value in [0,1] which acts as a discount factor to make states distant in the future less important. γ=1 means there is no discounting. γ=0.95 means that rewards lose 5% of their value each step into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x2rU83HRsAJ"
      },
      "outputs": [],
      "source": [
        "class MazeMDP():\n",
        "\n",
        "  MOVES: dict[int,tuple] = {\n",
        "         0: np.array([-1, 0]), #UP\n",
        "         1: np.array([1, 0]),  #DOWN\n",
        "         2: np.array([0, -1]), #LEFT\n",
        "         3: np.array([0, 1])   #RIGHT\n",
        "         }\n",
        "  UP: int = 0\n",
        "  DOWN: int = 1\n",
        "  LEFT: int = 2\n",
        "  RIGHT: int = 3\n",
        "\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "    \"\"\"Initialize the MDP\"\"\"\n",
        "    self.maze = np.array(maze)\n",
        "    assert len(self.maze.shape) == 2, \"The maze must be a 2-dimensional array\"\n",
        "\n",
        "    self.obstacles = [tuple(loc) for loc in np.argwhere(self.maze == 1)]\n",
        "    def get_state_index_from_maze(value):\n",
        "      locations = [tuple(loc) for loc in np.argwhere(self.maze == value)]\n",
        "      assert len(locations) == 1, f\"There should be exactly one {value} in the maze\"\n",
        "      row, col = locations[0]\n",
        "      return self.row_col_to_state(row, col)\n",
        "    self.init_state = get_state_index_from_maze(2)\n",
        "    self.goal_state = get_state_index_from_maze(3)\n",
        "\n",
        "    \"\"\"Create the MDP functions\"\"\"\n",
        "    self.S = set(range(self.maze.shape[0] * self.maze.shape[1]))\n",
        "    self.A = set(range(len(self.MOVES)))\n",
        "    self.R = self.build_rewards()\n",
        "    self.P = self.build_transitions()\n",
        "\n",
        "    self.H = H\n",
        "    self.gamma = gamma\n",
        "\n",
        "\n",
        "  def build_rewards(self):\n",
        "    \"\"\"Build reward function\"\"\"\n",
        "    # make every state and action and next state give -1 reward, to penalize long paths\n",
        "    rewards = np.full((len(self.S), len(self.A), len(self.S)), -1, dtype=float)\n",
        "    # all actions in the goal state give zero reward\n",
        "    # this is a requirement for all terminating states\n",
        "    rewards[self.goal_state, :, :] = 0\n",
        "\n",
        "    return rewards\n",
        "  def build_transitions(self):\n",
        "    \"\"\"Build transitoins function\"\"\"\n",
        "    transitions = np.zeros((len(self.S), len(self.A), len(self.S)))\n",
        "\n",
        "    # Go through all states and all actions\n",
        "    for s in self.S:\n",
        "      for a in self.A:\n",
        "        # By default, the next state is the current state\n",
        "        s_p = s\n",
        "\n",
        "        # Convert state to maze index\n",
        "        row_col = np.array(self.state_to_row_col(s))\n",
        "\n",
        "        # Get new potential state\n",
        "        row_col_new = row_col + self.MOVES[a]\n",
        "\n",
        "        # Check that new state is valid\n",
        "        is_in_bounds = 0 <= row_col_new[0] < self.maze.shape[0] and 0 <= row_col_new[1] < self.maze.shape[1]\n",
        "        is_free_space = tuple(row_col_new) not in self.obstacles\n",
        "\n",
        "        # Compute the new s' if the transition is valid\n",
        "        if is_in_bounds and is_free_space:\n",
        "          s_p = self.row_col_to_state(*row_col_new)\n",
        "\n",
        "        # Assign transition probability to 1\n",
        "        transitions[s,a,s_p] = 1\n",
        "\n",
        "    return transitions\n",
        "\n",
        "  def state_to_row_col(self, state):\n",
        "    \"\"\"A utility function to get the row and col from a state index\"\"\"\n",
        "    return state // self.maze.shape[1], state % self.maze.shape[1]\n",
        "\n",
        "  def row_col_to_state(self, row, col):\n",
        "    \"\"\"A utility function to get the state index from row and col\"\"\"\n",
        "    return row * self.maze.shape[1] + col"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gymnasium/Gym Standard Interface"
      ],
      "metadata": {
        "id": "9i4siVok_KtA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82glTHFDLc8n"
      },
      "source": [
        "Now that we've clearly defined the MDP, we want to provide an interface for an agent to interact with the environment. This is quite a simple environment, but you can imagine that they get quite complex. Gymnasium is a package that provides a common Environment interface between many such environments, and other useful utilities. `gymnasium` is often abbreviated as `gym`, because it is a well-maintained fork of OpenAI's Gym. A `gym.Env` has the following interface:\n",
        "- `observation_space`: The observation space defines what data types the states are represented by. For example, an integer or a vector of floats.\n",
        "- `action_space`: The action space defines what data types the actions are represented by. It is common to have an integer represent different choices.\n",
        "- `reset()` -> `observation, info`: The reset function marks the start of a new episode. It resets the environment back to its starting state. It returns the starting observation (and a dictionary of auxillary info).\n",
        "- `step(action)` -> `observation, reward, termination, truncation, info`: The step function executes an action chosen by the agent. The resulting state is returned along with the reward for that action. Termination and truncation signal that the current episode is done (such as by reaching the goal or time running out).\n",
        "- `render()`: Render is for visualization purposes and is optional.\n",
        "\n",
        "Let's see `gymnasium` in action with a few of their built-in environments. You can refer to https://gymnasium.farama.org/ for more details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run these if you try to use a Box2D environment\n",
        "# !pip install swig\n",
        "# !pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "2IbVGeYflWLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dBTsZrpQIXm"
      },
      "outputs": [],
      "source": [
        "# Jupyter notebook stuff to render the output\n",
        "image_widget = ipywidgets.Image()\n",
        "display(image_widget)\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# Lots of environments to choose from. Here are some examples\n",
        "# \"CartPole-v1\", \"MountainCar-v0\", \"Acrobot-v1\", \"LunarLander-v2\", \"CarRacing-v2\"\n",
        "environment_id = \"MountainCar-v0\"\n",
        "env = gym.make(environment_id, render_mode=\"rgb_array\")\n",
        "\n",
        "def agent_policy(observation):\n",
        "  # You can choose actions with any method.\n",
        "  # This just chooses a random action\n",
        "  return env.action_space.sample()\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "observation, info = env.reset()\n",
        "for _ in range(500):\n",
        "  action = agent_policy(observation)\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  done = terminated or truncated\n",
        "  if done:\n",
        "    # restart the episode when the previous finishes\n",
        "    observation, info = env.reset()\n",
        "\n",
        "  # Render\n",
        "  image = env.render()\n",
        "  image_widget.value = image_to_bytes(image)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OAEnQNrZ5iY"
      },
      "source": [
        "## Wrapping our MDP with Gym\n",
        "\n",
        "Now that you are familiar with what Gymnasium does, let's define a gym environment for our previously defined MazeMDP class. In most cases, you would just build the MDP directly into the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Gym Environment"
      ],
      "metadata": {
        "id": "MQZWniuC_Wt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhcNE1q6Iokn"
      },
      "outputs": [],
      "source": [
        "\"\"\"Define the Maze Gym Environment\"\"\"\n",
        "class MazeEnv(gym.Env):\n",
        "\n",
        "  def __init__(self, mdp, render_mode = \"ansi\"):\n",
        "    \"\"\"Initialize the maze environment setting all variables and parsing the map\"\"\"\n",
        "    self.mdp = mdp\n",
        "\n",
        "    self.agent_curr_state = self.mdp.init_state\n",
        "    self.steps_taken = 0\n",
        "\n",
        "    # Set action space and observation space\n",
        "    self.action_space = gym.spaces.Discrete(len(self.mdp.A))\n",
        "    self.observation_space = gym.spaces.Discrete(len(self.mdp.S))\n",
        "\n",
        "    # Rendering\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to its initial state\"\"\"\n",
        "    self.agent_curr_state = self.mdp.init_state\n",
        "    self.steps_taken = 0\n",
        "\n",
        "    initial_obs = self.agent_curr_state\n",
        "    initial_info = {}\n",
        "\n",
        "    return initial_obs, initial_info\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Take an action in the environment\"\"\"\n",
        "\n",
        "    assert 0 <= action < 4, \"Action should be one of [0,1,2,3]\"\n",
        "\n",
        "    prev_state = self.agent_curr_state\n",
        "    # Sample next state weighted by transition probabilities\n",
        "    self.agent_curr_state = np.random.choice(list(self.mdp.S), 1, p=self.mdp.P[self.agent_curr_state, action,:])[0]\n",
        "\n",
        "    # Get reward according to state and action and the resulting state\n",
        "    reward = self.mdp.R[prev_state, action, self.agent_curr_state]\n",
        "\n",
        "    # Make outputs for gym environment\n",
        "    next_obs = self.agent_curr_state\n",
        "\n",
        "    self.steps_taken += 1\n",
        "    has_reached_goal = self.agent_curr_state == self.mdp.goal_state\n",
        "    has_reached_time_limit = self.steps_taken >= self.mdp.H\n",
        "    terminated = has_reached_goal or has_reached_time_limit\n",
        "\n",
        "    # truncated indicates that the episode was ended due to some external condition\n",
        "    truncated = False\n",
        "    # info can contain auxillary information for logging and debugging\n",
        "    info = {}\n",
        "\n",
        "    return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "  def render(self):\n",
        "    \"\"\" Render the environment \"\"\"\n",
        "\n",
        "    if self.render_mode == \"ansi\":\n",
        "      # ansi is text mode\n",
        "      return render_text(self.agent_curr_state, self.mdp.maze)\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported rendering mode {self.render_mode}\")\n",
        "\n",
        "def render_text(agent_state, maze):\n",
        "  \"\"\"Get a text form of the maze\"\"\"\n",
        "\n",
        "  # These are the characters that will render in your maze.\n",
        "  # 0 = \"-\" is empty space\n",
        "  # 1 = \"X\" is a wall\n",
        "  # 2 = \"-\" is the agent starting location. However, it can be rendered as empty space\n",
        "  # 3 = \"G\" is the goal location\n",
        "  # 4 = \"T\" is a teleporter (you will implement this)\n",
        "  # extend this list if you add more features\n",
        "  character_map = [\"-\", \"X\", \"-\", \"G\", \"T\"]\n",
        "  ncols, nrows = maze.shape\n",
        "\n",
        "  string=f\"\"\n",
        "  for i in range(ncols):\n",
        "    for j in range(nrows):\n",
        "      s = (i * ncols + j)\n",
        "      if s == agent_state:\n",
        "        # The agent is displayed as \"A\"\n",
        "        string += \"A\"\n",
        "      else:\n",
        "        string += character_map[maze[i, j]]\n",
        "    string += \"\\n\"\n",
        "\n",
        "  return string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7TZHk6jmd8F"
      },
      "source": [
        "### Random Agent in the Maze\n",
        "We can test out this environment with an agent that selects random actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Cn1N2jevmF"
      },
      "outputs": [],
      "source": [
        "STEPS_PER_SECOND = 10\n",
        "output = ipywidgets.Output()\n",
        "display(output)\n",
        "\n",
        "# Create the maze MDP. 0 is free space, 1 is walls,\n",
        "# 2 is the starting location, 3 is the goal location\n",
        "mdp = MazeMDP([[2,0,0,0,0],\n",
        "               [1,1,1,1,0],\n",
        "               [0,0,0,0,0],\n",
        "               [0,1,0,1,1],\n",
        "               [0,1,0,0,3]])\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(mdp)\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done:\n",
        "  # sample a random action\n",
        "  action = maze_env.action_space.sample()\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Agent in the Maze\n",
        "Below, we provide the code for running Value Iteration, a method for solving an MDP. We can use it to test the effects of different reward functions on the Agent's behavior. We will learn how this works later, but the agent will calculate how good each state is for reaching the goal.\n",
        "\n",
        "You can think of this as the agent considering every possible action in every state. It does this planning in advance, so that when the agent starts moving, it just follows the path that is calculated."
      ],
      "metadata": {
        "id": "kz6LAffJCTIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIterationAgent():\n",
        "  \"\"\"Agent class for solving tabular MDPs\"\"\"\n",
        "\n",
        "  def __init__(self, env):\n",
        "    \"\"\"Initialize the agent. env should have a tabular mdp\"\"\"\n",
        "\n",
        "    assert hasattr(env, 'mdp'), \"Gymansium Environment does not have associated MDP\"\n",
        "\n",
        "    self.env = env\n",
        "    self.mdp = env.mdp\n",
        "    self.policy = None\n",
        "    self.v = None\n",
        "\n",
        "  def value_improvement(self, epsilon=.000001):\n",
        "    \"\"\"Perform value improvement\"\"\"\n",
        "    mdp = self.mdp\n",
        "\n",
        "    # Randomly initialize v\n",
        "    # Easiest initialization is all zeros\n",
        "    v = np.zeros(len(mdp.S))\n",
        "\n",
        "    # Loop until the horizon is reached\n",
        "    for i in range(mdp.H):\n",
        "\n",
        "      old_v = v\n",
        "\n",
        "      # Perform Bellman Backup\n",
        "      v = np.max(\n",
        "            np.sum(\n",
        "                mdp.P * (mdp.R + mdp.gamma * v[np.newaxis, np.newaxis, :]), axis=-1\n",
        "            )\n",
        "          , axis=-1)\n",
        "      # Set goal state value to 0\n",
        "      v[mdp.goal_state] = 0\n",
        "\n",
        "      # Exit if threshold is reached\n",
        "      if i > 30 and np.max(np.abs(old_v-v)) < epsilon:\n",
        "        break\n",
        "\n",
        "    # Determine the best actions with bellman backups\n",
        "    action_reward = np.sum(mdp.P * (mdp.R + mdp.gamma * v[np.newaxis, np.newaxis, :]), axis=-1)\n",
        "\n",
        "    # Compute the optimal policy\n",
        "    best_actions = (action_reward - np.max(action_reward, axis=1)[:,np.newaxis]) == 0\n",
        "    policy = best_actions * (1 / np.sum( best_actions, axis=1)[:,np.newaxis])\n",
        "\n",
        "    self.policy = policy\n",
        "    self.v = v\n",
        "\n",
        "    return policy, v\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\"Get the action given by the policy\"\"\"\n",
        "\n",
        "    # If policy is none, return a random action\n",
        "    if self.policy is None:\n",
        "      print(\"Warning: Value iteration policy has not been computed yet\")\n",
        "      return self.env.action_space.sample()\n",
        "\n",
        "    # Get the action given by the policy\n",
        "    action = np.random.choice(list(self.mdp.A), 1, p=self.policy[state,:])[0]\n",
        "\n",
        "    return action\n",
        "\n",
        "  def get_value(self, state):\n",
        "    \"\"\"Get the value of a given state\"\"\"\n",
        "\n",
        "    return self.v[state]\n",
        "\n",
        "  def get_value_heatmap(self, agent_state, output_shape=(256,256)):\n",
        "    \"\"\"Gets a matrix of all the state values\"\"\"\n",
        "    value_matrix = self.v.reshape((self.mdp.maze.shape))\n",
        "    # arr = np.nan_to_num(arr, nan=10000)\n",
        "    for loc in self.mdp.obstacles:\n",
        "      value_matrix[loc] = np.nan\n",
        "    image = matrix_to_heatmap_image(value_matrix, output_shape)\n",
        "    # draw agent\n",
        "    agent_frac_x = (agent_state // self.mdp.maze.shape[1] + 0.5) / self.mdp.maze.shape[0]\n",
        "    agent_frac_y = (agent_state % self.mdp.maze.shape[1] + 0.5) / self.mdp.maze.shape[1]\n",
        "    x_half_width = output_shape[0] / self.mdp.maze.shape[0] * 0.25\n",
        "    y_half_width = output_shape[1] / self.mdp.maze.shape[1] * 0.25\n",
        "    min_x = int(output_shape[0] * agent_frac_x - x_half_width)\n",
        "    max_x = int(output_shape[0] * agent_frac_x + x_half_width)\n",
        "    min_y = int(output_shape[1] * agent_frac_y - y_half_width)\n",
        "    max_y = int(output_shape[1] * agent_frac_y + y_half_width)\n",
        "    image[min_x:max_x, min_y:max_y,:] = 255\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "VBcA6G5GCRvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run the same maze to see what the optimal agent does."
      ],
      "metadata": {
        "id": "2OawI2HemviT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_SECOND = 2\n",
        "output = ipywidgets.Output()\n",
        "image_widget = ipywidgets.Image()\n",
        "display(image_widget)\n",
        "display(output)\n",
        "\n",
        "# Create our MDP to specify the maze\n",
        "mdp = MazeMDP([[2,0,0,0,0],\n",
        "               [1,1,1,1,0],\n",
        "               [0,0,0,0,0],\n",
        "               [0,1,0,1,1],\n",
        "               [0,1,0,0,3]])\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(mdp)\n",
        "\n",
        "# Create and call a value iteration agent\n",
        "agent = ValueIterationAgent(maze_env)\n",
        "agent.value_improvement()\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done and num_steps < 100:\n",
        "  # Get best action from the agent policy\n",
        "  action = agent.get_action(obs)\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "  value_heatmap = agent.get_value_heatmap(obs)\n",
        "  image_widget.value = image_to_bytes(value_heatmap)\n",
        "\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps\")"
      ],
      "metadata": {
        "id": "wOpNHDNNmGtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands On Exploration"
      ],
      "metadata": {
        "id": "lRXiODfGtHUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Function\n",
        "\n",
        "Now, let's do some guided exploration with a new reward function.\n",
        "\n",
        "1. Try a reward function with 1 for transitions into the goal state, `R[:, :, Goal] = 1` and zero for everything else. Does this reward function change the agent's behavior as you change gamma between 0 and 1? (try these values of gamma: 0, 0.5, 0.95, 1)\n",
        "  * Refer to the MazeMDP code for an example of how to implement the reward function.\n",
        "2. Make a reward function of your own. Try to see if you can make the agent take a different path to the goal or demonstrate interesting behavior. For example, you can try these ideas:\n",
        "  *   Give a negative reward every time the agent moves up\n",
        "  *   Give a negative reward every time the agent moves left\n",
        "  *   Give a negative reward if the agent has a certain x or y coordinate\n",
        "3. What can happen if you change your reward function to give positive rewards each time a certain action is taken? Why is this?\n",
        "\n",
        "4. Try moving around the agent starting location and the goal location.\n",
        "\n"
      ],
      "metadata": {
        "id": "RrxESexxCkWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AltRewardMazeMDP(MazeMDP):\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "    super().__init__(maze, H=H, gamma=gamma)\n",
        "\n",
        "  def build_rewards(self):\n",
        "    \"\"\"Build reward function\"\"\"\n",
        "    # This gives you the original reward function. You can use it or not.\n",
        "    rewards = super().build_rewards()\n",
        "\n",
        "    # self.xy_to_state(my_x, my_y) is a handy utility function to get the state index from a position\n",
        "    # You can refer to the actions as self.UP, self.DOWN, self.LEFT, self.RIGHT\n",
        "    # example: rewards[self.xy_to_state(2,1), self.UP, :] = -10 gives -10 reward when the agent is at x=2, y=1 and moves up (action 0)\n",
        "\n",
        "    # do stuff here\n",
        "\n",
        "    return rewards\n",
        "\n",
        "test_maze = [[2,0,0,0,0],\n",
        "             [1,1,1,1,0],\n",
        "             [0,0,0,0,0],\n",
        "             [0,1,0,1,1],\n",
        "             [0,1,0,0,3]]\n",
        "\n",
        "test_maze_2 = [[2,0,0,0,0,1,0,0,0,0],\n",
        "               [1,1,1,1,0,1,0,1,0,1],\n",
        "               [0,0,0,0,0,1,0,1,0,0],\n",
        "               [0,1,1,1,0,1,0,1,0,1],\n",
        "               [0,0,0,1,0,0,0,1,0,0],\n",
        "               [0,1,0,0,0,0,1,0,0,0],\n",
        "               [0,1,1,1,1,1,1,1,0,1],\n",
        "               [0,0,0,1,0,0,0,0,0,3],\n",
        "               [0,1,0,0,0,0,1,1,1,0],\n",
        "               [0,1,0,0,1,0,0,0,0,0],]\n",
        "\n"
      ],
      "metadata": {
        "id": "zmKfMTZNCxnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide testing code to see how your the Value Iteration Agent interacts with your environment after finding the optimal actions for the MDP. Play around with it."
      ],
      "metadata": {
        "id": "iRt55k-vEsUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_SECOND = 2\n",
        "output = ipywidgets.Output()\n",
        "image_widget = ipywidgets.Image()\n",
        "display(image_widget)\n",
        "display(output)\n",
        "\n",
        "# Create the alternate reward MDP. Select a maze.\n",
        "gamma = 0.95\n",
        "maze = test_maze_2\n",
        "mdp = AltRewardMazeMDP(maze, gamma=gamma)\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(mdp)\n",
        "\n",
        "# Create and call a value iteration agent\n",
        "agent = ValueIterationAgent(maze_env)\n",
        "agent.value_improvement()\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "total_reward = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done and num_steps < 40:\n",
        "  # Get best action from the agent policy\n",
        "  action = agent.get_action(obs)\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "  total_reward += reward\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "    print(f'Action: {[\"Up\",\"Down\",\"Left\",\"Right\"][action]} ({action})')\n",
        "  value_heatmap = agent.get_value_heatmap(obs)\n",
        "  image_widget.value = image_to_bytes(value_heatmap)\n",
        "\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps with {total_reward} reward\")"
      ],
      "metadata": {
        "id": "eOStUrfEEnuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transitions\n",
        "For some fun, we are going to now play around with modifying the environment transitions by adding a transportation square. When the agent takes an action that would move it into a transportation square, it gets moved to another transportation square on the map at random. This is called a **stochastic** environment, where the same action in the same location can produce a different result. Fortunately, our Value Iteration agent can already handle the complexity.\n",
        "\n",
        "We specify a teleport state on the map with the number '4'."
      ],
      "metadata": {
        "id": "XAU11BFQIFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeleporterMazeMDP(MazeMDP):\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "\n",
        "    self.teleport_states = [tuple(loc) for loc in np.argwhere(np.array(maze) == 4)]\n",
        "    print(self.teleport_states)\n",
        "    super().__init__(maze, H=H, gamma=gamma)\n",
        "\n",
        "\n",
        "\n",
        "  def build_transitions(self):\n",
        "    \"\"\"Build transitoins function\"\"\"\n",
        "    transitions = np.zeros((len(self.S), len(self.A), len(self.S)))\n",
        "\n",
        "    # Go through all states and all actions\n",
        "    for s in self.S:\n",
        "      for a in self.A:\n",
        "        # By default, the next state is the current state\n",
        "        s_p = s\n",
        "\n",
        "        # Convert state to maze index\n",
        "        row_col = np.array(self.state_to_row_col(s))\n",
        "\n",
        "        # Get new potential state\n",
        "        row_col_new = row_col + self.MOVES[a]\n",
        "\n",
        "        # Check that new state is valid\n",
        "        is_in_bounds = 0 <= row_col_new[0] < self.maze.shape[0] and 0 <= row_col_new[1] < self.maze.shape[1]\n",
        "        is_free_space = tuple(row_col_new) not in self.obstacles\n",
        "\n",
        "        # Compute the new s' if the transition is valid\n",
        "        if is_in_bounds and is_free_space:\n",
        "          s_p = self.row_col_to_state(row_col_new[0],row_col_new[1])\n",
        "\n",
        "        if tuple(row_col_new) not in self.teleport_states:\n",
        "          # If we have not move to a teleporter, assign transition probability to 1\n",
        "          transitions[s,a,s_p] = 1\n",
        "\n",
        "        else:\n",
        "          # Define the logic for the if agent is in a teleport state\n",
        "\n",
        "          # Get the flat index of the teleport states\n",
        "          tp_states = np.array([self.row_col_to_state(*s) for s in self.teleport_states])\n",
        "\n",
        "          # Get how many teleport states there are\n",
        "\n",
        "          # Define a uniform probability of moving to each teleport state\n",
        "\n",
        "          # Assign that uniform probability to transitions[s,a,tp_states]\n",
        "          transitions[s,a,tp_states] = 1/(len(tp_states))\n",
        "\n",
        "    return transitions\n",
        "\n",
        "\n",
        "tp_test_maze = [[2,0,0,0,0],\n",
        "                [1,1,1,1,4],\n",
        "                [0,0,0,0,0],\n",
        "                [0,1,0,1,1],\n",
        "                [0,4,0,0,3]]\n",
        "\n",
        "tp_test_maze_2 = [[2,0,0,0,0,1,0,0,0,0],\n",
        "                  [1,1,1,1,0,1,0,1,0,1],\n",
        "                  [0,0,0,0,0,1,0,1,0,4],\n",
        "                  [0,1,1,1,0,1,0,1,0,1],\n",
        "                  [0,0,4,1,0,0,0,1,0,0],\n",
        "                  [0,1,0,0,0,0,1,0,0,0],\n",
        "                  [0,1,1,1,1,1,1,1,0,1],\n",
        "                  [0,0,0,1,0,0,0,0,0,3],\n",
        "                  [0,1,0,0,0,0,1,1,1,0],\n",
        "                  [4,1,0,0,1,0,4,0,0,0],]"
      ],
      "metadata": {
        "id": "y2bZGIR4IDCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide code to test your TeleporterMDP environment. Notice how the way we wrote the Gym Environment requires *no changes* even though we are using a different underlying MDP. This is the power of the Gym API!\n",
        "\n",
        "1. How does the optimal policy change with the number and location of teleporting squares in the map? If there is 1 square? 2 squares? 4 squares?\n",
        "\n",
        "2. What happens if you change the probability of teleporting?\n",
        "\n",
        "3. Try to make teleporting optional. To do this, add a 5th action (id=4) that will only teleport the agent to another teleporting square if that action is taken. In non-teleporting squares the agent will remain in the same square when taking the teleporting action. How does this change the optimal policy?\n",
        "\n",
        "4. Does changing the reward function change the utilty of teleporting squares? What if there is a cost associated with a teleporting square? To test this, add the build_rewards() function to the TeleporterMDP and implement it with a different reward function."
      ],
      "metadata": {
        "id": "FsRwtOCPKjoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_SECOND = 2\n",
        "output = ipywidgets.Output()\n",
        "image_widget = ipywidgets.Image()\n",
        "display(image_widget)\n",
        "display(output)\n",
        "\n",
        "# Create the alternate teleportation MDP. Select a maze.\n",
        "gamma = 0.99\n",
        "maze = tp_test_maze\n",
        "test_mdp = TeleporterMazeMDP(maze, gamma=gamma )\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(test_mdp)\n",
        "\n",
        "# Create and call a value iteration agent\n",
        "agent = ValueIterationAgent(maze_env)\n",
        "agent.value_improvement()\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "total_reward = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done and num_steps < 40:\n",
        "  # Get best action from the agent policy\n",
        "  action = agent.get_action(obs)\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "  total_reward += reward\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "    print(f'Action: {[\"Up\",\"Down\",\"Left\",\"Right\",\"Teleport\"][action]} ({action})')\n",
        "  value_heatmap = agent.get_value_heatmap(obs)\n",
        "  image_widget.value = image_to_bytes(value_heatmap)\n",
        "\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps with {total_reward} reward\")"
      ],
      "metadata": {
        "id": "3Utbxag-KkHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further exploration! A few more ideas if you are interested:\n",
        "\n",
        "1. Add a lava tile where the agent incurs a large negative reward for touching the lava tile. When does the agent avoid the lava tiles? Or not? How does this depend on the reward function and the discount factor (gamma)?\n",
        "\n",
        "2. Add more actions. Allow the agent to move along the diagonal. This should be a straightforward extension (add more moves to the MOVES array!)"
      ],
      "metadata": {
        "id": "8ZJXzUSFLyOK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}