{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9jtNGqiIKZW"
   },
   "source": [
    "This Google Colab notebook is made to help guide students through the concept of a Markov Decision Process (MDP) using the example of a grid maze. It also covers setup of a Gymnasium Environment and an implementation of Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G60-chhbH_BW",
    "outputId": "e7ef095c-097b-4bd9-e44d-bfeca5a02f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swig\n",
      "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
      "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: swig\n",
      "Successfully installed swig-4.2.1\n",
      "Collecting gymnasium[box2d,classic-control]\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d,classic-control])\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (2.6.1)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d,classic-control])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (4.2.1)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376098 sha256=24421b05a5196b4215b01b4a3b762729b6edc839f31ae5fa4b231dad0dee0785\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
      "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Install requisite packages\"\"\"\n",
    "!pip install swig\n",
    "!pip install gymnasium[box2d,classic-control]\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02tdipXSHZHc"
   },
   "outputs": [],
   "source": [
    "\"\"\"Import packages\"\"\"\n",
    "import io\n",
    "import time\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"Utility functions\"\"\"\n",
    "def image_to_bytes(image):\n",
    "  img = Image.fromarray(image, 'RGB')\n",
    "\n",
    "  # Convert the PIL image to bytes\n",
    "  with io.BytesIO() as output:\n",
    "    img.save(output, format=\"PNG\")\n",
    "    image_data = output.getvalue()\n",
    "  return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxAv0b_nBmfO"
   },
   "source": [
    "We want to represent a maze environment, where there is an agent that can move in the cardinal directions. There are walls which restrict movement. For a given maze of size WIDTHxHEIGHT, our MDP is defined as\n",
    "- S: The state space is {0, 1, 2, ..., WIDTH*HEIGHT-1}, representing the column-wise flattened indices of the maze. For example, for a 5x5 maze, state 8 represents x=1, y=3.\n",
    "- A: The action space is {0, 1, 2, 3}, representing the directions UP, DOWN, LEFT, and RIGHT respectively.\n",
    "- R(S, A) -> ℝ The reward function takes a state and an action and gives a reward. For our simple problem, we give a constant penalty of -1 every action the agent takes. No reward is given at the goal.\n",
    "- P(S, A, S) -> [0,1] The transition function shows the probability of starting at a given state, taking a certain action, and ending up in a different state. Our maze environment is deterministic, so all probabilities are either 0 or 1. For any non-neighbor states S1 and S2, P(S1, ⋅, S2)=0. But if state S3 is to the left of S4, then P(S3, LEFT, S4)=1.\n",
    "- H: The horizon is the maximum amount of actions our agent is allowed to take.\n",
    "- γ (gamma): Gamma is a value in [0,1] which acts as a discount factor to make states distant in the future less important. γ=1 means there is no discounting. γ=0.95 means that rewards lose 5% of their value each step into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x2rU83HRsAJ"
   },
   "outputs": [],
   "source": [
    "class MazeMDP():\n",
    "\n",
    "  MOVES: dict[int,tuple] = {\n",
    "         0: np.array([-1, 0]), #UP\n",
    "         1: np.array([1, 0]),  #DOWN\n",
    "         2: np.array([0, -1]), #LEFT\n",
    "         3: np.array([0, 1])   #RIGHT\n",
    "         }\n",
    "\n",
    "  def __init__(self, maze, H=100, gamma=.95):\n",
    "    \"\"\"Initialize the MDP\"\"\"\n",
    "    self.maze = np.array(maze)\n",
    "\n",
    "    self.obstacles = [tuple(loc) for loc in np.argwhere(self.maze == 1)]\n",
    "    def get_state_index_from_maze(value):\n",
    "      locations = [tuple(loc) for loc in np.argwhere(self.maze == value)]\n",
    "      assert len(locations) == 1, f\"There should be exactly one {value} in the maze\"\n",
    "      x, y = locations[0]\n",
    "      return x * self.maze.shape[0] + y\n",
    "    self.init_state = get_state_index_from_maze(2)\n",
    "    self.goal_state = get_state_index_from_maze(3)\n",
    "\n",
    "    \"\"\"Create the MDP functions\"\"\"\n",
    "    self.S = set(range(self.maze.shape[0] * self.maze.shape[1]))\n",
    "    self.A = set(range(len(self.MOVES)))\n",
    "    self.R = self.build_rewards()\n",
    "    self.P = self.build_transitions()\n",
    "\n",
    "    self.H = H\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def build_rewards(self):\n",
    "    \"\"\"Build reward function\"\"\"\n",
    "    # make every state and action give -1 reward, to penalize long paths\n",
    "    rewards = np.full((len(self.S), len(self.A)), -1)\n",
    "    # all actions in the goal state give zero reward\n",
    "    rewards[self.goal_state, :] = 0\n",
    "\n",
    "    return rewards\n",
    "\n",
    "  def build_transitions(self):\n",
    "    \"\"\"Build transitoins function\"\"\"\n",
    "    transitions = np.zeros((len(self.S), len(self.A), len(self.S)))\n",
    "\n",
    "    # Go through all states and all actions\n",
    "    for s in self.S:\n",
    "      for a in self.A:\n",
    "        # By default, the next state is the current state\n",
    "        s_p = s\n",
    "\n",
    "        # Convert state to maze index\n",
    "        xy = np.array([s // self.maze.shape[1], s % self.maze.shape[1]])\n",
    "\n",
    "        # Get new potential state\n",
    "        xy_new = xy + self.MOVES[a]\n",
    "\n",
    "        # Check that new state is valid\n",
    "        is_in_bounds = 0 <= xy_new[0] < self.maze.shape[0] and 0 <= xy_new[1] < self.maze.shape[1]\n",
    "        is_free_space = tuple(xy_new) not in self.obstacles\n",
    "\n",
    "        # Compute the new s' if the transition is valid\n",
    "        if is_in_bounds and is_free_space:\n",
    "          s_p = xy_new[0] * self.maze.shape[1] + xy_new[1]\n",
    "\n",
    "        # Assign transition probability to 1\n",
    "        transitions[s,a,s_p] = 1\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82glTHFDLc8n"
   },
   "source": [
    "Now that we've clearly defined the MDP, we want to provide an interface for an agent to interact with the environment. This is quite a simple environment, but you can imagine that they get quite complex. Gymnasium is a package that provides a common Environment interface between many such environments, and other useful utilities. `gymnasium` is often abbreviated as `gym`, because it is a well-maintained fork of OpenAI's Gym. A `gym.Env` has the following interface:\n",
    "- `observation_space`: The observation space defines what data types the states are represented by. For example, an integer or a vector of floats.\n",
    "- `action_space`: The action space defines what data types the actions are represented by. It is common to have an integer represent different choices.\n",
    "- `reset()` -> `observation, info`: The reset function marks the start of a new episode. It resets the environment back to its starting state. It returns the starting observation (and a dictionary of auxillary info).\n",
    "- `step(action)` -> `observation, reward, termination, truncation, info`: The step function executes an action chosen by the agent. The resulting state is returned along with the reward for that action. Termination and truncation signal that the current episode is done (such as by reaching the goal or time running out).\n",
    "- `render()`: Render is for visualization purposes and is optional.\n",
    "\n",
    "Let's see `gymnasium` in action with a few of their built-in environments. You can refer to https://gymnasium.farama.org/ for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514,
     "referenced_widgets": [
      "f452f269599c47759add06027e13133e",
      "4c10f4171b544712a7644cb5485f2ee5"
     ]
    },
    "id": "4dBTsZrpQIXm",
    "outputId": "fcab064f-fb0b-4db8-e6fe-ca6ba01c467a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n",
      "Action space: Discrete(4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f452f269599c47759add06027e13133e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Lots of environments to choose from. Here are some examples\n",
    "# \"CartPole-v1\", \"MountainCar-v0\", \"Acrobot-v1\", \"LunarLander-v2\", \"CarRacing-v2\"\n",
    "environment_id = \"LunarLander-v2\"\n",
    "env = gym.make(environment_id, render_mode=\"rgb_array\")\n",
    "\n",
    "def agent_policy(observation):\n",
    "  # You can choose actions with any method.\n",
    "  # This just chooses a random action\n",
    "  return env.action_space.sample()\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "# Jupyter notebook stuff to render the output\n",
    "image_widget = ipywidgets.Image()\n",
    "display(image_widget)\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "  action = agent_policy(observation)\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  done = terminated or truncated\n",
    "  if done:\n",
    "    # restart the episode when the previous finishes\n",
    "    observation, info = env.reset()\n",
    "\n",
    "  # Render\n",
    "  image = env.render()\n",
    "  image_widget.value = image_to_bytes(image)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OAEnQNrZ5iY"
   },
   "source": [
    "Now that you are familiar with what Gymnasium does, let's define a gym environment for our previously defined MazeMDP class. In most cases, you would just build the MDP directly into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhcNE1q6Iokn"
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the Maze Gym Environment\"\"\"\n",
    "class MazeEnv(gym.Env):\n",
    "\n",
    "  def __init__(self, mdp, render_mode = \"ansi\"):\n",
    "    \"\"\"Initialize the maze environment setting all variables and parsing the map\"\"\"\n",
    "    self.mdp = mdp\n",
    "\n",
    "    self.agent_curr_state = self.mdp.init_state\n",
    "    self.steps_taken = 0\n",
    "\n",
    "    # Set action space and observation space\n",
    "    self.action_space = gym.spaces.Discrete(len(self.mdp.A))\n",
    "    self.observation_space = gym.spaces.Discrete(len(self.mdp.S))\n",
    "\n",
    "    # Rendering\n",
    "    self.render_mode = render_mode\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Reset the environment to its initial state\"\"\"\n",
    "    self.agent_curr_state = self.mdp.init_state\n",
    "    self.steps_taken = 0\n",
    "\n",
    "    initial_obs = self.agent_curr_state\n",
    "    initial_info = {}\n",
    "\n",
    "    return initial_obs, initial_info\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Take an action in the environment\"\"\"\n",
    "\n",
    "    assert 0 <= action < 4, \"Action should be one of [0,1,2,3]\"\n",
    "\n",
    "    # Get reward according to state and action\n",
    "    reward = self.mdp.R[self.agent_curr_state, action]\n",
    "\n",
    "    # Sample next state weighted by transition probabilities\n",
    "    self.agent_curr_state = np.random.choice(list(self.mdp.S), 1, p=self.mdp.P[self.agent_curr_state, action,:])[0]\n",
    "\n",
    "    # Make outputs for gym environment\n",
    "    next_obs = self.agent_curr_state\n",
    "\n",
    "    self.steps_taken += 1\n",
    "    has_reached_goal = self.agent_curr_state == self.mdp.goal_state\n",
    "    has_reached_time_limit = self.steps_taken >= self.mdp.H\n",
    "    terminated = has_reached_goal or has_reached_time_limit\n",
    "\n",
    "    # truncated indicates that the episode was ended due to some external condition\n",
    "    truncated = False\n",
    "    # info can contain auxillary information for logging and debugging\n",
    "    info = {}\n",
    "\n",
    "    return next_obs, reward, terminated, truncated, info\n",
    "\n",
    "  def render(self):\n",
    "    \"\"\" Render the environment \"\"\"\n",
    "\n",
    "    if self.render_mode == \"ansi\":\n",
    "      # ansi is text mode\n",
    "      return render_text(self.agent_curr_state, self.mdp.maze)\n",
    "    else:\n",
    "      raise ValueError(f\"Unsupported rendering mode {self.render_mode}\")\n",
    "\n",
    "def render_text(agent_state, maze):\n",
    "  \"\"\"Get a text form of the maze\"\"\"\n",
    "  character_map = [\"-\", \"X\", \"-\", \"G\"]\n",
    "  ncols, nrows = maze.shape\n",
    "\n",
    "  string=f\"\"\n",
    "  for i in range(ncols):\n",
    "    for j in range(nrows):\n",
    "      s = (i * ncols + j)\n",
    "      if s == agent_state:\n",
    "        string += \"A\"\n",
    "      else:\n",
    "        string += character_map[maze[i, j]]\n",
    "    string += \"\\n\"\n",
    "\n",
    "  return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7TZHk6jmd8F"
   },
   "source": [
    "We can test out this environment with an agent that selects random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159,
     "referenced_widgets": [
      "1624e0f5b818483d8e34dd45e8c118f1",
      "8c7cc97f01bd43c7a71629b6cead8d50"
     ]
    },
    "id": "o4Cn1N2jevmF",
    "outputId": "e5133aae-646b-40e6-cad0-050c3e6e5b90"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1624e0f5b818483d8e34dd45e8c118f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 100 steps\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_SECOND = 10\n",
    "output = ipywidgets.Output()\n",
    "display(output)\n",
    "\n",
    "# Create the maze MDP. 0 is free space, 1 is walls,\n",
    "# 2 is the starting location, 3 is the goal location\n",
    "mdp = MazeMDP([[2,0,0,0,0],\n",
    "               [1,1,1,1,0],\n",
    "               [0,0,0,0,0],\n",
    "               [0,1,0,1,1],\n",
    "               [0,1,0,0,3]])\n",
    "\n",
    "# Create the maze gym environment\n",
    "maze_env = MazeEnv(mdp)\n",
    "\n",
    "terminated = truncated = done = False\n",
    "obs, info = maze_env.reset()\n",
    "num_steps = 0\n",
    "while not done:\n",
    "  # sample a random action\n",
    "  action = maze_env.action_space.sample()\n",
    "\n",
    "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
    "  done = terminated or truncated\n",
    "  obs = next_obs\n",
    "\n",
    "  # Render\n",
    "  with output:\n",
    "    clear_output(wait=True)\n",
    "    print(maze_env.render())\n",
    "  time.sleep(1/STEPS_PER_SECOND)\n",
    "  num_steps += 1\n",
    "\n",
    "print(f\"Finished in {num_steps} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gahPnrngjAUL"
   },
   "source": [
    "Clearly the random agent had no idea what it was doing. Using the Value Iteration algorithm, we can find the optimal policy to choose the best actions. We will discuss more about how this algorithm works in future sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s7NWwbHaZmS"
   },
   "outputs": [],
   "source": [
    "class ValueIterationAgent():\n",
    "  \"\"\"Agent class for solving tabular MDPs\"\"\"\n",
    "\n",
    "  def __init__(self, env):\n",
    "    \"\"\"Initialize the agent. env should have a tabular mdp\"\"\"\n",
    "\n",
    "    assert hasattr(env, 'mdp'), \"Gymansium Environment does not have associated MDP\"\n",
    "\n",
    "    self.env = env\n",
    "    self.mdp = env.mdp\n",
    "    self.policy = None\n",
    "    self.v = None\n",
    "\n",
    "\n",
    "  def value_improvement(self, epsilon=.01):\n",
    "    \"\"\"Perform value improvement\"\"\"\n",
    "    mdp = self.mdp\n",
    "\n",
    "    # Randomly initialize v\n",
    "    # Easiest initialization is all zeros\n",
    "    v = np.zeros(len(mdp.S))\n",
    "\n",
    "    # Loop until the horizon is reached\n",
    "    for _ in range(mdp.H):\n",
    "\n",
    "      old_v = v\n",
    "\n",
    "      # Perform Bellman Backup\n",
    "      v = np.max(np.sum( mdp.P * (mdp.R[:,:,np.newaxis] + mdp.gamma * v), axis=-1), axis=-1)\n",
    "      # Set goal state value to 0\n",
    "      v[mdp.goal_state] = 0\n",
    "\n",
    "      # Exit if threshold is reached\n",
    "      if np.max(np.abs(old_v-v)) < epsilon:\n",
    "        break\n",
    "\n",
    "    # Determine the best actions with bellman backups\n",
    "    action_reward = np.sum(mdp.P * (mdp.R[:,:,np.newaxis] + mdp.gamma * v), axis=-1)\n",
    "\n",
    "    # Compute the optimal policy\n",
    "    best_actions = (action_reward - np.max(action_reward, axis=1)[:,np.newaxis]) == 0\n",
    "    policy = best_actions * (1 / np.sum( best_actions, axis=1)[:,np.newaxis])\n",
    "\n",
    "    self.policy = policy\n",
    "    self.v = v\n",
    "\n",
    "    return policy, v\n",
    "\n",
    "  def get_action(self, state):\n",
    "    \"\"\"Get the action given by the policy\"\"\"\n",
    "\n",
    "    # If policy is none, return a random action\n",
    "    if self.policy is None:\n",
    "      print(\"Warning: Value iteration policy has not been computed yet\")\n",
    "      return self.env.action_space.sample()\n",
    "\n",
    "    # Get the action given by the policy\n",
    "    action = np.random.choice(list(self.mdp.A), 1, p=self.policy[state,:])[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "  def get_value(self, state):\n",
    "    \"\"\"Get the value of a given state\"\"\"\n",
    "\n",
    "    return self.v[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG5LiegRk-AY"
   },
   "source": [
    "Now we can try again with the ValueIterationAgent. You'll find that it solves it reliably and much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439,
     "referenced_widgets": [
      "943d4b4c1e2141268cdc0eaacd90013d",
      "d288cd85efc24d1e94459b1fd1f6a788"
     ]
    },
    "id": "PjcL6dNgl6gQ",
    "outputId": "4222c42b-f115-4046-c077-94a95bf1d121"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943d4b4c1e2141268cdc0eaacd90013d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in  11\n",
      "Finished in  12\n",
      "Finished in  12\n",
      "Finished in  12\n",
      "Finished in  12\n",
      "Finished in  12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-59c88fa25abf>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSTEPS_PER_SECOND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mnum_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEPS_PER_SECOND = 2\n",
    "output = ipywidgets.Output()\n",
    "display(output)\n",
    "\n",
    "# Create the maze MDP. 0 is free space, 1 is walls,\n",
    "# 2 is the starting location, 3 is the goal location\n",
    "mdp = MazeMDP([[2,0,0,0,0],\n",
    "               [1,1,1,1,0],\n",
    "               [0,0,0,0,0],\n",
    "               [0,1,0,1,1],\n",
    "               [0,1,0,0,3]])\n",
    "\n",
    "# Create the maze gym environment\n",
    "maze_env = MazeEnv(mdp)\n",
    "\n",
    "# Create and call a value iteration agent\n",
    "agent = ValueIterationAgent(maze_env)\n",
    "agent.value_improvement()\n",
    "\n",
    "terminated = truncated = done = False\n",
    "obs, info = maze_env.reset()\n",
    "num_steps = 0\n",
    "while not done:\n",
    "  # sample a random action\n",
    "  action = agent.get_action(obs)\n",
    "\n",
    "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
    "  done = terminated or truncated\n",
    "  obs = next_obs\n",
    "\n",
    "  if done:\n",
    "    obs, info = maze_env.reset()\n",
    "    done = False\n",
    "    print(\"Finished in \", num_steps)\n",
    "    num_steps = 0\n",
    "\n",
    "  # Render\n",
    "  with output:\n",
    "    clear_output(wait=True)\n",
    "    print(maze_env.render())\n",
    "  time.sleep(1/STEPS_PER_SECOND)\n",
    "  num_steps += 1\n",
    "\n",
    "print(f\"Finished in {num_steps} steps\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1624e0f5b818483d8e34dd45e8c118f1": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_8c7cc97f01bd43c7a71629b6cead8d50",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "-----\n",
         "XXXXA\n",
         "-----\n",
         "-X-XX\n",
         "-X--G\n",
         "\n"
        ]
       }
      ]
     }
    },
    "4c10f4171b544712a7644cb5485f2ee5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c7cc97f01bd43c7a71629b6cead8d50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "943d4b4c1e2141268cdc0eaacd90013d": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_d288cd85efc24d1e94459b1fd1f6a788",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "-A---\n",
         "XXXX-\n",
         "-----\n",
         "-X-XX\n",
         "-X--G\n",
         "\n"
        ]
       }
      ]
     }
    },
    "d288cd85efc24d1e94459b1fd1f6a788": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f452f269599c47759add06027e13133e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ImageModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ImageModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ImageView",
      "format": "png",
      "height": "",
      "layout": "IPY_MODEL_4c10f4171b544712a7644cb5485f2ee5",
      "width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
