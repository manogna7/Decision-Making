{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jtNGqiIKZW"
      },
      "source": [
        "This Google Colab notebook is made to help guide students through the concept of a Markov Decision Process (MDP) using the example of a grid maze. It also covers setup of a Gymnasium Environment and an implementation of Value Iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G60-chhbH_BW",
        "outputId": "69f20b86-494c-42da-88db-fa23d376a181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting gymnasium[box2d,classic-control]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d,classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (2.6.1)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d,classic-control])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d,classic-control]) (4.2.1)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376093 sha256=2ae8808c87c5642a01fd7f5f2f1e2a02ebb78dc3d85b4f1355ecda1d5f40c51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Install requisite packages\"\"\"\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d,classic-control]\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02tdipXSHZHc"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import packages\"\"\"\n",
        "import io\n",
        "import time\n",
        "\n",
        "import ipywidgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\"\"\"Utility functions\"\"\"\n",
        "def image_to_bytes(image):\n",
        "  img = Image.fromarray(image, 'RGB')\n",
        "\n",
        "  # Convert the PIL image to bytes\n",
        "  with io.BytesIO() as output:\n",
        "    img.save(output, format=\"PNG\")\n",
        "    image_data = output.getvalue()\n",
        "  return image_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAv0b_nBmfO"
      },
      "source": [
        "We want to represent a maze environment, where there is an agent that can move in the cardinal directions. There are walls which restrict movement. For a given maze of size WIDTHxHEIGHT, our MDP is defined as\n",
        "- S: The state space is {0, 1, 2, ..., WIDTH*HEIGHT-1}, representing the column-wise flattened indices of the maze. For example, for a 5x5 maze, state 8 represents x=1, y=3.\n",
        "- A: The action space is {0, 1, 2, 3}, representing the directions UP, DOWN, LEFT, and RIGHT respectively.\n",
        "- R(S, A) -> ℝ The reward function takes a state and an action and gives a reward. For our simple problem, we give a constant penalty of -1 every action the agent takes. No reward is given at the goal.\n",
        "- P(S, A, S) -> [0,1] The transition function shows the probability of starting at a given state, taking a certain action, and ending up in a different state. Our maze environment is deterministic, so all probabilities are either 0 or 1. For any non-neighbor states S1 and S2, P(S1, ⋅, S2)=0. But if state S3 is to the left of S4, then P(S3, LEFT, S4)=1.\n",
        "- H: The horizon is the maximum amount of actions our agent is allowed to take.\n",
        "- γ (gamma): Gamma is a value in [0,1] which acts as a discount factor to make states distant in the future less important. γ=1 means there is no discounting. γ=0.95 means that rewards lose 5% of their value each step into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x2rU83HRsAJ"
      },
      "outputs": [],
      "source": [
        "class MazeMDP():\n",
        "\n",
        "  MOVES: dict[int,tuple] = {\n",
        "         0: np.array([-1, 0]), #UP\n",
        "         1: np.array([1, 0]),  #DOWN\n",
        "         2: np.array([0, -1]), #LEFT\n",
        "         3: np.array([0, 1])   #RIGHT\n",
        "         }\n",
        "\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "    \"\"\"Initialize the MDP\"\"\"\n",
        "    self.maze = np.array(maze)\n",
        "\n",
        "    self.obstacles = [tuple(loc) for loc in np.argwhere(self.maze == 1)]\n",
        "    def get_state_index_from_maze(value):\n",
        "      locations = [tuple(loc) for loc in np.argwhere(self.maze == value)]\n",
        "      assert len(locations) == 1, f\"There should be exactly one {value} in the maze\"\n",
        "      x, y = locations[0]\n",
        "      return x * self.maze.shape[0] + y\n",
        "    self.init_state = get_state_index_from_maze(2)\n",
        "    self.goal_state = get_state_index_from_maze(3)\n",
        "\n",
        "    \"\"\"Create the MDP functions\"\"\"\n",
        "    self.S = set(range(self.maze.shape[0] * self.maze.shape[1]))\n",
        "    self.A = set(range(len(self.MOVES)))\n",
        "    self.R = self.build_rewards()\n",
        "    self.P = self.build_transitions()\n",
        "\n",
        "    self.H = H\n",
        "    self.gamma = gamma\n",
        "\n",
        "\n",
        "  def build_rewards(self):\n",
        "    \"\"\"Build reward function\"\"\"\n",
        "    # make every state and action give -1 reward, to penalize long paths\n",
        "    rewards = np.full((len(self.S), len(self.A)), -1)\n",
        "    # all actions in the goal state give zero reward\n",
        "    rewards[self.goal_state, :] = 0\n",
        "\n",
        "    return rewards\n",
        "  def build_transitions(self):\n",
        "    \"\"\"Build transitoins function\"\"\"\n",
        "    transitions = np.zeros((len(self.S), len(self.A), len(self.S)))\n",
        "\n",
        "    # Go through all states and all actions\n",
        "    for s in self.S:\n",
        "      for a in self.A:\n",
        "        # By default, the next state is the current state\n",
        "        s_p = s\n",
        "\n",
        "        # Convert state to maze index\n",
        "        xy = np.array([s // self.maze.shape[1], s % self.maze.shape[1]])\n",
        "\n",
        "        # Get new potential state\n",
        "        xy_new = xy + self.MOVES[a]\n",
        "\n",
        "        # Check that new state is valid\n",
        "        is_in_bounds = 0 <= xy_new[0] < self.maze.shape[0] and 0 <= xy_new[1] < self.maze.shape[1]\n",
        "        is_free_space = tuple(xy_new) not in self.obstacles\n",
        "\n",
        "        # Compute the new s' if the transition is valid\n",
        "        if is_in_bounds and is_free_space:\n",
        "          s_p = xy_new[0] * self.maze.shape[1] + xy_new[1]\n",
        "\n",
        "        # Assign transition probability to 1\n",
        "        transitions[s,a,s_p] = 1\n",
        "\n",
        "    return transitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82glTHFDLc8n"
      },
      "source": [
        "Now that we've clearly defined the MDP, we want to provide an interface for an agent to interact with the environment. This is quite a simple environment, but you can imagine that they get quite complex. Gymnasium is a package that provides a common Environment interface between many such environments, and other useful utilities. `gymnasium` is often abbreviated as `gym`, because it is a well-maintained fork of OpenAI's Gym. A `gym.Env` has the following interface:\n",
        "- `observation_space`: The observation space defines what data types the states are represented by. For example, an integer or a vector of floats.\n",
        "- `action_space`: The action space defines what data types the actions are represented by. It is common to have an integer represent different choices.\n",
        "- `reset()` -> `observation, info`: The reset function marks the start of a new episode. It resets the environment back to its starting state. It returns the starting observation (and a dictionary of auxillary info).\n",
        "- `step(action)` -> `observation, reward, termination, truncation, info`: The step function executes an action chosen by the agent. The resulting state is returned along with the reward for that action. Termination and truncation signal that the current episode is done (such as by reaching the goal or time running out).\n",
        "- `render()`: Render is for visualization purposes and is optional.\n",
        "\n",
        "Let's see `gymnasium` in action with a few of their built-in environments. You can refer to https://gymnasium.farama.org/ for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "06db4800c6a443b0b44b37081eef6c21",
            "09f10009896546c99590f3bc21684ada"
          ]
        },
        "id": "4dBTsZrpQIXm",
        "outputId": "1078d75b-3a4c-46e4-8605-462ceab4b65b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Action space: Discrete(2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Image(value=b'')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06db4800c6a443b0b44b37081eef6c21"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# Lots of environments to choose from. Here are some examples\n",
        "# \"CartPole-v1\", \"MountainCar-v0\", \"Acrobot-v1\", \"LunarLander-v2\", \"CarRacing-v2\"\n",
        "environment_id = \"CartPole-v1\"\n",
        "env = gym.make(environment_id, render_mode=\"rgb_array\")\n",
        "\n",
        "def agent_policy(observation):\n",
        "  # You can choose actions with any method.\n",
        "  # This just chooses a random action\n",
        "  return env.action_space.sample()\n",
        "\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n",
        "# Jupyter notebook stuff to render the output\n",
        "image_widget = ipywidgets.Image()\n",
        "display(image_widget)\n",
        "\n",
        "observation, info = env.reset()\n",
        "for _ in range(500):\n",
        "  action = agent_policy(observation)\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  done = terminated or truncated\n",
        "  if done:\n",
        "    # restart the episode when the previous finishes\n",
        "    observation, info = env.reset()\n",
        "\n",
        "  # Render\n",
        "  image = env.render()\n",
        "  image_widget.value = image_to_bytes(image)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OAEnQNrZ5iY"
      },
      "source": [
        "Now that you are familiar with what Gymnasium does, let's define a gym environment for our previously defined MazeMDP class. In most cases, you would just build the MDP directly into the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhcNE1q6Iokn"
      },
      "outputs": [],
      "source": [
        "\"\"\"Define the Maze Gym Environment\"\"\"\n",
        "class MazeEnv(gym.Env):\n",
        "\n",
        "  def __init__(self, mdp, render_mode = \"ansi\"):\n",
        "    \"\"\"Initialize the maze environment setting all variables and parsing the map\"\"\"\n",
        "    self.mdp = mdp\n",
        "\n",
        "    self.agent_curr_state = self.mdp.init_state\n",
        "    self.steps_taken = 0\n",
        "\n",
        "    # Set action space and observation space\n",
        "    self.action_space = gym.spaces.Discrete(len(self.mdp.A))\n",
        "    self.observation_space = gym.spaces.Discrete(len(self.mdp.S))\n",
        "\n",
        "    # Rendering\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to its initial state\"\"\"\n",
        "    self.agent_curr_state = self.mdp.init_state\n",
        "    self.steps_taken = 0\n",
        "\n",
        "    initial_obs = self.agent_curr_state\n",
        "    initial_info = {}\n",
        "\n",
        "    return initial_obs, initial_info\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Take an action in the environment\"\"\"\n",
        "\n",
        "    assert 0 <= action < 4, \"Action should be one of [0,1,2,3]\"\n",
        "\n",
        "    # Get reward according to state and action\n",
        "    reward = self.mdp.R[self.agent_curr_state, action]\n",
        "\n",
        "    # Sample next state weighted by transition probabilities\n",
        "    self.agent_curr_state = np.random.choice(list(self.mdp.S), 1, p=self.mdp.P[self.agent_curr_state, action,:])[0]\n",
        "\n",
        "    # Make outputs for gym environment\n",
        "    next_obs = self.agent_curr_state\n",
        "\n",
        "    self.steps_taken += 1\n",
        "    has_reached_goal = self.agent_curr_state == self.mdp.goal_state\n",
        "    has_reached_time_limit = self.steps_taken >= self.mdp.H\n",
        "    terminated = has_reached_goal or has_reached_time_limit\n",
        "\n",
        "    # truncated indicates that the episode was ended due to some external condition\n",
        "    truncated = False\n",
        "    # info can contain auxillary information for logging and debugging\n",
        "    info = {}\n",
        "\n",
        "    return next_obs, reward, terminated, truncated, info\n",
        "\n",
        "  def render(self):\n",
        "    \"\"\" Render the environment \"\"\"\n",
        "\n",
        "    if self.render_mode == \"ansi\":\n",
        "      # ansi is text mode\n",
        "      return render_text(self.agent_curr_state, self.mdp.maze)\n",
        "    else:\n",
        "      raise ValueError(f\"Unsupported rendering mode {self.render_mode}\")\n",
        "\n",
        "def render_text(agent_state, maze):\n",
        "  \"\"\"Get a text form of the maze\"\"\"\n",
        "  character_map = [\"-\", \"X\", \"-\", \"G\"]\n",
        "  ncols, nrows = maze.shape\n",
        "\n",
        "  string=f\"\"\n",
        "  for i in range(ncols):\n",
        "    for j in range(nrows):\n",
        "      s = (i * ncols + j)\n",
        "      if s == agent_state:\n",
        "        string += \"A\"\n",
        "      else:\n",
        "        string += character_map[maze[i, j]]\n",
        "    string += \"\\n\"\n",
        "\n",
        "  return string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7TZHk6jmd8F"
      },
      "source": [
        "We can test out this environment with an agent that selects random actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "427d666410ef4b5d94661bd42648753c",
            "0df18c28c5b44da7a130d585008c8865"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "o4Cn1N2jevmF",
        "outputId": "c101c923-d21c-451a-ef6d-66299ea8755e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "427d666410ef4b5d94661bd42648753c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished in 100 steps\n"
          ]
        }
      ],
      "source": [
        "STEPS_PER_SECOND = 10\n",
        "output = ipywidgets.Output()\n",
        "display(output)\n",
        "\n",
        "# Create the maze MDP. 0 is free space, 1 is walls,\n",
        "# 2 is the starting location, 3 is the goal location\n",
        "mdp = MazeMDP([[2,0,0,0,0],\n",
        "               [1,1,1,1,0],\n",
        "               [0,0,0,0,0],\n",
        "               [0,1,0,1,1],\n",
        "               [0,1,0,0,3]])\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(mdp)\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done:\n",
        "  # sample a random action\n",
        "  action = maze_env.action_space.sample()\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide the code for running Value Iteration, a method for solving an MDP. We can use it to test the effects of different reward functions on the Agent's behavior."
      ],
      "metadata": {
        "id": "kz6LAffJCTIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIterationAgent():\n",
        "  \"\"\"Agent class for solving tabular MDPs\"\"\"\n",
        "\n",
        "  def __init__(self, env):\n",
        "    \"\"\"Initialize the agent. env should have a tabular mdp\"\"\"\n",
        "\n",
        "    assert hasattr(env, 'mdp'), \"Gymansium Environment does not have associated MDP\"\n",
        "\n",
        "    self.env = env\n",
        "    self.mdp = env.mdp\n",
        "    self.policy = None\n",
        "    self.v = None\n",
        "\n",
        "\n",
        "  def value_improvement(self, epsilon=.01):\n",
        "    \"\"\"Perform value improvement\"\"\"\n",
        "    mdp = self.mdp\n",
        "\n",
        "    # Randomly initialize v\n",
        "    # Easiest initialization is all zeros\n",
        "    v = np.zeros(len(mdp.S))\n",
        "\n",
        "    # Loop until the horizon is reached\n",
        "    for _ in range(mdp.H):\n",
        "\n",
        "      old_v = v\n",
        "\n",
        "      # Perform Bellman Backup\n",
        "      v = np.max(np.sum( mdp.P * (mdp.R[:,:,np.newaxis] + mdp.gamma * v), axis=-1), axis=-1)\n",
        "      # Set goal state value to 0\n",
        "      v[mdp.goal_state] = 0\n",
        "\n",
        "      # Exit if threshold is reached\n",
        "      if np.max(np.abs(old_v-v)) < epsilon:\n",
        "        break\n",
        "\n",
        "    # Determine the best actions with bellman backups\n",
        "    action_reward = np.sum(mdp.P * (mdp.R[:,:,np.newaxis] + mdp.gamma * v), axis=-1)\n",
        "\n",
        "    # Compute the optimal policy\n",
        "    best_actions = (action_reward - np.max(action_reward, axis=1)[:,np.newaxis]) == 0\n",
        "    policy = best_actions * (1 / np.sum( best_actions, axis=1)[:,np.newaxis])\n",
        "\n",
        "    self.policy = policy\n",
        "    self.v = v\n",
        "\n",
        "    return policy, v\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\"Get the action given by the policy\"\"\"\n",
        "\n",
        "    # If policy is none, return a random action\n",
        "    if self.policy is None:\n",
        "      print(\"Warning: Value iteration policy has not been computed yet\")\n",
        "      return self.env.action_space.sample()\n",
        "\n",
        "    # Get the action given by the policy\n",
        "    action = np.random.choice(list(self.mdp.A), 1, p=self.policy[state,:])[0]\n",
        "\n",
        "    return action\n",
        "\n",
        "  def get_value(self, state):\n",
        "    \"\"\"Get the value of a given state\"\"\"\n",
        "\n",
        "    return self.v[state]"
      ],
      "metadata": {
        "id": "VBcA6G5GCRvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do some guided exploration with a new reward function.\n",
        "\n",
        "1. Try a reward function with all zeros for all actions and 1 in the goal state. Does this reward function change the agent's behavior as you change gamma between 0 and 1?\n",
        "2. Refer to the MazeMDP code for an example of how to implement the reward function.\n",
        "3. Make a reward function of your own. Does this reward function solve the maze optimally? Under what conditions (goal location, agent starting location) does the agent move to the goal in the least number of steps?\n",
        "\n",
        "\n",
        "*   Give a negative reward every time the agent moves down\n",
        "*   Give a negative reward every time the agent moves right\n",
        "*   Change the location of the agent so that it has to move up and right to get to the goal.\n",
        "* What reward do you need to assign to the goal for the agent to still reach it?\n",
        "* Does this change if you make the map bigger?\n",
        "\n",
        "4. Can you find a reward function that generally allows the agent to solve the maze in the least number of steps? (Hint: look at the original implementation!)\n",
        "\n"
      ],
      "metadata": {
        "id": "RrxESexxCkWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AltRewardMazeMDP(MazeMDP):\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "    super().__init__(maze, H=H, gamma=gamma)\n",
        "\n",
        "  def build_rewards(self):\n",
        "    \"\"\"Build reward function\"\"\"\n",
        "    #raise NotImplementedError(\"Please implement this :) \")\n",
        "    return\n",
        "\n",
        "test_maze = [[2,0,0,0,0],\n",
        "             [1,1,1,1,0],\n",
        "             [0,0,0,0,0],\n",
        "             [0,1,0,1,1],\n",
        "             [0,1,0,0,3]]\n",
        "\n",
        "test_maze_2 = [[2,0,0,0,0,1,0,0,0,0],\n",
        "               [1,1,1,1,0,1,1,1,0,1],\n",
        "               [0,0,0,0,0,1,0,1,0,0],\n",
        "               [0,1,0,1,1,0,0,0,0,1],\n",
        "               [0,1,0,1,0,0,0,1,0,0],\n",
        "               [0,1,0,0,0,0,1,0,0,0],\n",
        "               [0,1,0,1,1,1,1,1,1,1],\n",
        "               [0,0,0,1,0,0,0,0,0,0],\n",
        "               [0,1,0,0,0,0,1,1,1,0],\n",
        "               [0,1,0,1,0,1,3,0,0,0],]\n",
        "\n",
        "gamma = 0.99\n",
        "test_mdp = AltRewardMazeMDP(test_maze_2, gamma=gamma )\n",
        "\n"
      ],
      "metadata": {
        "id": "zmKfMTZNCxnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide testing code to see how your Maze Agent interacts with the environment after solving the MDP with value iteration. Is it solving the Maze optimally? Why or why not?"
      ],
      "metadata": {
        "id": "iRt55k-vEsUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_SECOND = 2\n",
        "output = ipywidgets.Output()\n",
        "display(output)\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(test_mdp)\n",
        "\n",
        "# Create and call a value iteration agent\n",
        "agent = ValueIterationAgent(maze_env)\n",
        "agent.value_improvement()\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done and num_steps < 1000:\n",
        "  # Get best action from the agent policy\n",
        "  action = agent.get_action(obs)\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "6060528d6795447f87a7ad31188aafbb",
            "c3fcb8c9000946399095f62e2d75e92d"
          ]
        },
        "id": "eOStUrfEEnuG",
        "outputId": "5c0d2a24-c32c-4dcd-f82b-d504ee30faaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6060528d6795447f87a7ad31188aafbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished in 27 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some fun, we are going to now play around with modifying the environment transitions by adding a transportation square. When the agent takes an action from a transportation square, it gets moved to another transportation square on the map at random.\n",
        "\n",
        "We specify a teleport state on the map with the number '4'."
      ],
      "metadata": {
        "id": "XAU11BFQIFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeleporterMazeMDP(MazeMDP):\n",
        "  def __init__(self, maze, H=100, gamma=.95):\n",
        "    super().__init__(maze, H=H, gamma=gamma)\n",
        "\n",
        "    self.teleport_states = [tuple(loc) for loc in np.argwhere(self.maze == 4)]\n",
        "\n",
        "  def build_transitions(self):\n",
        "    \"\"\"Build transitoins function\"\"\"\n",
        "    transitions = np.zeros((len(self.S), len(self.A), len(self.S)))\n",
        "\n",
        "    raise NotImplementedError(\"Please implement this :)\")\n",
        "\n",
        "    # Go through all states and all actions\n",
        "    for s in self.S:\n",
        "      for a in self.A:\n",
        "        # By default, the next state is the current state\n",
        "        s_p = s\n",
        "\n",
        "        # Convert state to maze index\n",
        "        xy = np.array([s // self.maze.shape[1], s % self.maze.shape[1]])\n",
        "\n",
        "        # Define the logic for if the agent is not in a teleport state\n",
        "        if xy not in self.teleport_states:\n",
        "\n",
        "          # Get new potential state\n",
        "          xy_new = xy + self.MOVES[a]\n",
        "\n",
        "          # Check that new state is valid\n",
        "          is_in_bounds = 0 <= xy_new[0] < self.maze.shape[0] and 0 <= xy_new[1] < self.maze.shape[1]\n",
        "          is_free_space = tuple(xy_new) not in self.obstacles\n",
        "\n",
        "          # Compute the new s' if the transition is valid\n",
        "          if is_in_bounds and is_free_space:\n",
        "            s_p = xy_new[0] * self.maze.shape[1] + xy_new[1]\n",
        "\n",
        "          # Assign transition probability to 1\n",
        "          transitions[s,a,s_p] = 1\n",
        "\n",
        "        else:\n",
        "          # Define the logic for the if agent is in a teleport state\n",
        "\n",
        "          # Get the flat index of the teleport states\n",
        "          tp_states = np.array([s[0]*self.maze.shape[1] + s[1] for s in self.teleport_states])\n",
        "\n",
        "          # Get how many teleport states there are\n",
        "\n",
        "          # Define a uniform probability of moving to each teleport state\n",
        "\n",
        "          # Assign that uniform probability to transitions[s,a,tp_states]\n",
        "\n",
        "    return transitions\n",
        "\n",
        "\n",
        "tp_test_maze = [[2,0,0,0,0],\n",
        "                [1,1,1,4,0],\n",
        "                [4,0,0,0,0],\n",
        "                [0,1,0,1,1],\n",
        "                [0,4,0,0,3]]\n",
        "\n",
        "tp_test_maze_2 = [[2,0,0,0,0,1,0,0,0,0],\n",
        "                  [1,1,4,1,0,1,1,4,0,1],\n",
        "                  [0,0,0,0,0,1,0,1,0,0],\n",
        "                  [0,1,0,1,1,0,0,0,0,1],\n",
        "                  [0,1,0,1,4,0,0,1,0,4],\n",
        "                  [0,1,0,0,0,0,1,0,0,0],\n",
        "                  [0,1,0,1,1,1,1,1,1,1],\n",
        "                  [0,0,0,1,0,0,0,0,0,0],\n",
        "                  [0,4,0,0,0,4,1,1,4,0],\n",
        "                  [0,1,0,1,0,1,3,0,0,0],]\n",
        "\n",
        "gamma = 0.99\n",
        "test_mdp = TeleporterMazeMDP(tp_test_maze_2, gamma=gamma )"
      ],
      "metadata": {
        "id": "y2bZGIR4IDCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we provide code to test your TeleporterMDP environment. Notice how the way we wrote the Gym Environment requires *no changes* eventhough we are using a different underlying MDP. This is the power of the Gym API!\n",
        "\n",
        "1. How does the optimal policy change with the number and location of teleporting squares in the map? If there is 1 square? 2 squares? 4 squares?\n",
        "\n",
        "2. What happens if you change the probability of teleporting?\n",
        "\n",
        "3. Try to make teleporting optional. To do this, add a 5th action (id=4) that will only teleport the agent to another teleporting square if that action is taken. In non-teleporting squares the agent will remain in the same square when taking the teleporting action. How does this change the optimal policy?\n",
        "\n",
        "4. Does changing the reward function change the utilty of teleporting squares? What if there is a cost associated with a teleporting square? To test this, add the build_rewards() function to the TeleporterMDP and implement it with a different reward function."
      ],
      "metadata": {
        "id": "FsRwtOCPKjoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_SECOND = 2\n",
        "output = ipywidgets.Output()\n",
        "display(output)\n",
        "\n",
        "# Create the maze gym environment\n",
        "maze_env = MazeEnv(test_mdp)\n",
        "\n",
        "# Create and call a value iteration agent\n",
        "agent = ValueIterationAgent(maze_env)\n",
        "agent.value_improvement()\n",
        "\n",
        "terminated = truncated = done = False\n",
        "obs, info = maze_env.reset()\n",
        "num_steps = 0\n",
        "\n",
        "# Run the maze environment until the episode terminates\n",
        "while not done and num_steps < 1000:\n",
        "  # Get best action from the agent policy\n",
        "  action = agent.get_action(obs)\n",
        "\n",
        "  next_obs, reward, terminated, truncated, info = maze_env.step(action)\n",
        "  done = terminated or truncated\n",
        "  obs = next_obs\n",
        "\n",
        "  # Render\n",
        "  with output:\n",
        "    clear_output(wait=True)\n",
        "    print(maze_env.render())\n",
        "  time.sleep(1/STEPS_PER_SECOND)\n",
        "  num_steps += 1\n",
        "\n",
        "print(f\"Finished in {num_steps} steps\")"
      ],
      "metadata": {
        "id": "3Utbxag-KkHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further exploration! A few more ideas if you are interested:\n",
        "\n",
        "1. Add a lava tile where the agent incurs a large negative reward for touching the lava tile. When does the agent avoid the lava tiles? Or not? How does this depend on the reward function and the discount factor (gamma)?\n",
        "\n",
        "2. Add more actions. Allow the agent to move along the diagonal. This should be a straightforward extension (add more moves to the MOVES array!)"
      ],
      "metadata": {
        "id": "8ZJXzUSFLyOK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06db4800c6a443b0b44b37081eef6c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ImageModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ImageModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ImageView",
            "format": "png",
            "height": "",
            "layout": "IPY_MODEL_09f10009896546c99590f3bc21684ada",
            "width": ""
          }
        },
        "09f10009896546c99590f3bc21684ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "427d666410ef4b5d94661bd42648753c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_0df18c28c5b44da7a130d585008c8865",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "-A---\n",
                  "XXXX-\n",
                  "-----\n",
                  "-X-XX\n",
                  "-X--G\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "0df18c28c5b44da7a130d585008c8865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6060528d6795447f87a7ad31188aafbb": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c3fcb8c9000946399095f62e2d75e92d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "-----X----\n",
                  "XXXX-XXX-X\n",
                  "-----X----\n",
                  "-X-XX----X\n",
                  "-X-X---X--\n",
                  "-X----X---\n",
                  "-X-XXXXXXX\n",
                  "---X------\n",
                  "-X----XXX-\n",
                  "-X-X-XA---\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "c3fcb8c9000946399095f62e2d75e92d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}